{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/r0g06z5/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/r0g06z5/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config path\n",
    "config_path = 'config.json'\n",
    "\n",
    "# path for stored distributions\n",
    "path_word_topic_dist = 'word_topic_distributions.json'\n",
    "path_doc_topic_dist = 'doc_topic_distributions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "config = load_config(config_path)\n",
    "n_docs = config['n_docs']\n",
    "n_topics = config['n_topics']\n",
    "seed = config['seed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentences..\n",
      "\n",
      "Number of Documents: 15\n",
      "Number of Tokens: 6078\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "brown = nltk.corpus.brown\n",
    "docs = np.random.choice(brown.fileids(), n_docs, replace=False)\n",
    "\n",
    "print('Processing sentences..\\n')\n",
    "processed_docs = {}\n",
    "for doc in docs:\n",
    "    processed_sents = []\n",
    "    sents = brown.sents(doc)\n",
    "    for sent in sents:\n",
    "        processed_sents.append([word.lower() for word in sent if word.isalnum() and word not in stopwords])\n",
    "    processed_docs[doc] = processed_sents\n",
    "\n",
    "processed_sents = list(itertools.chain(*list(processed_docs.values())))\n",
    "final_tokens = list(set(itertools.chain(*processed_sents)))\n",
    "n_tokens = len(final_tokens)\n",
    "\n",
    "token2int = dict(zip(final_tokens, range(n_tokens)))\n",
    "int2token = {v:k for k,v in token2int.items()}\n",
    "\n",
    "doc2int = dict(zip(docs, range(n_docs)))\n",
    "int2doc = {v:k for k,v in doc2int.items()}\n",
    "\n",
    "print('Number of Documents:', n_docs) \n",
    "print('Number of Tokens:', n_tokens)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Model of LDA\n",
    "\n",
    "After inferring the distributions $\\beta_{WT}$ (word-topic distribution) and $\\theta_{DT}$ (doc-topic distribution), we can use these to sample lot of words, therby generating documents. The approach is as follows:\n",
    "\n",
    "Consider building a new document similar to document $D$:\n",
    "1. Sample a topic from $\\theta_{D}$. Let this be $T$.\n",
    "2. Sample a word $W$ from $\\beta_{T}$.\n",
    "3. Repeat 1 and 2, for however many words you need in the new document.\n",
    "\n",
    "If you want to make a new document using all documents, uniformly sample a document $D$ and repeat the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_distributions(path_word_topic_dist, path_doc_topic_dist):\n",
    "    with open(path_word_topic_dist) as f:\n",
    "        beta = json.load(f)\n",
    "    with open(path_doc_topic_dist) as f:\n",
    "        theta = json.load(f)\n",
    "    return beta, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generative_model(processed_docs, beta, theta):\n",
    "    generated_docs = {}\n",
    "    for doc, sents in processed_docs.items():\n",
    "        n_words = len(list(itertools.chain(*sents)))\n",
    "        generated_doc = []\n",
    "\n",
    "        # sampling topics from document multinomial distribution\n",
    "        topic_counts = np.random.multinomial(n_words, theta[doc])\n",
    "\n",
    "        for topic_idx, count in enumerate(topic_counts):\n",
    "            # sampling words from topic multinomial distribution\n",
    "            count_dist = np.random.multinomial(count, beta[str(topic_idx)])\n",
    "            word_idxs = np.where(count_dist>0)[0]\n",
    "            counts = count_dist[word_idxs]\n",
    "\n",
    "            # build the document by populating words\n",
    "            word_idxs = word_idxs.reshape(-1,1) \n",
    "            word_idxs = list(map(lambda x, y: list(x) * y, word_idxs, counts))\n",
    "            word_idxs = list(itertools.chain(*word_idxs))            \n",
    "            generated_doc += word_idxs\n",
    "\n",
    "        # check generated doc length equal to original doc length\n",
    "        assert len(generated_doc) == n_words\n",
    "\n",
    "        # convert word indices to words\n",
    "        generated_doc = list(map(lambda x: int2token[x], generated_doc))\n",
    "\n",
    "        # shuffle words in document\n",
    "        np.random.shuffle(generated_doc)\n",
    "        generated_docs[doc] = generated_doc\n",
    "    return generated_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distributions..\n",
      "Generating documents with probabilistic model..\n"
     ]
    }
   ],
   "source": [
    "print('Loading distributions..')\n",
    "beta, theta = load_distributions(path_word_topic_dist, path_doc_topic_dist)\n",
    "\n",
    "print('Generating documents with probabilistic model..')\n",
    "generated_docs = generative_model(processed_docs, beta, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of LDA\n",
    "\n",
    "**Compute document similarities** \n",
    "\n",
    "Compute **Kullback-Leibler Divergence (KL Divergence)** between their topic distributions. KL Divergence between distributions $p$ and $q$ is given by:\n",
    "\n",
    "$$\n",
    "D[p(x): q(x)]=\\sum_{x} p(x) \\log \\frac{p(x)}{q(x)}\n",
    "$$\n",
    "\n",
    "As KL Divergence is asymmetric, as $D(p, q)$ $\\neq$ $D(q, p)$, Symmetrized KL Divergence is given by:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}[D(p, q)+D(q, p)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(p, q):\n",
    "    return np.sum(p * np.log(p/q))\n",
    "\n",
    "def sym_KL_divergence(p, q):\n",
    "    return (KL_divergence(p, q) + KL_divergence(q, p)) / 2\n",
    "\n",
    "def get_document_similarities(theta):\n",
    "    doc_similarity = np.zeros((n_docs, n_docs))\n",
    "    for doc1, theta_doc1 in theta.items():\n",
    "        doc_idx1 = doc2int[doc1]\n",
    "        for doc2, theta_doc2 in theta.items():\n",
    "            doc_idx2 = doc2int[doc2]\n",
    "            doc_similarity[doc_idx1, doc_idx2] = sym_KL_divergence(np.array(theta_doc1), np.array(theta_doc2))\n",
    "    return doc_similarity\n",
    "\n",
    "def get_most_similar_documents(doc_similarity, INFTY_SIM):\n",
    "    np.fill_diagonal(doc_similarity, INFTY_SIM)\n",
    "    idxs = np.dstack(np.unravel_index(np.argsort(doc_similarity.ravel()), doc_similarity.shape))[0][0]\n",
    "    idxs = list(map(lambda x: int2doc[x], idxs))\n",
    "    return idxs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document similarities with KL Divergence..\n",
      "Most similar documents are \"cc14\" and \"ck21\"\n"
     ]
    }
   ],
   "source": [
    "print('Document similarities with KL Divergence..')\n",
    "doc_similarity = get_document_similarities(theta)\n",
    "\n",
    "INFTY_SIM = 1000\n",
    "idxs = get_most_similar_documents(doc_similarity, INFTY_SIM)\n",
    "print(f'Most similar documents are \"{idxs[0]}\" and \"{idxs[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of LDA\n",
    "\n",
    "Information Retrieval - **Similarity with respect to a query**\n",
    "\n",
    "Given a query $q$, we want to calculate the conditional probability given that it's coming from a document $d_{i}$, and do so for all documents:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&p\\left(q \\mid d_{i}\\right)=\\prod_{w_{k} \\in q} p\\left(w_{k} \\mid d_{i}\\right) \\\\\n",
    "&=\\prod_{w_{k} \\in q_{j=1}} \\sum_{j=1}^{T} P\\left(w_{k} \\mid z=j\\right) P\\left(z=j \\mid d_{i}\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_doc_similarity(n_query, beta, theta, seed):\n",
    "    np.random.seed(seed)\n",
    "    query = np.random.choice(final_tokens, n_query, replace=True)\n",
    "    print('Given query:', query)\n",
    "    query_idxs = list(map(lambda x: token2int[x], query))\n",
    "\n",
    "    cond_probs = []\n",
    "    for doc in docs:\n",
    "        theta_doc = theta[doc]\n",
    "        cond_prob = 1\n",
    "        for word_idx in query_idxs:\n",
    "            word_prob = 0\n",
    "            for topic_idx in range(n_topics):\n",
    "                p1 = beta[str(topic_idx)][word_idx]\n",
    "                p2 = theta_doc[topic_idx]\n",
    "                word_prob += p1 * p2\n",
    "            cond_prob *= word_prob\n",
    "        cond_probs.append(cond_prob)\n",
    "    \n",
    "    # normalize probabilities\n",
    "    cond_probs = np.array(cond_probs) / sum(cond_probs)\n",
    "    return cond_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given query: ['invitation' 'check' 'infinite' 'strasny' 'jack' 'overseas' 'van'\n",
      " 'rolled' 'follow' 'permit']\n",
      "\n",
      "Most similar doc to given query is \"cf15\"\n"
     ]
    }
   ],
   "source": [
    "n_query = 10\n",
    "cond_probs = get_query_doc_similarity(n_query, beta, theta, seed)\n",
    "most_similar_doc_idx = np.where(cond_probs == np.max(cond_probs))[0][0]\n",
    "most_similar_doc = int2doc[most_similar_doc_idx]\n",
    "\n",
    "print(f'\\nMost similar doc to given query is \"{most_similar_doc}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of LDA\n",
    "\n",
    "**Similarity between two words**\n",
    "\n",
    "Given two words $w_{1}$ and $w_{2}$, compute similarity between them:\n",
    "$$\n",
    "p\\left(w_{2} \\mid w_{1}\\right)=\\sum_{j=1}^{T} p\\left(w_{2} \\mid z=j\\right) p\\left(z=j \\mid w_{i}\\right)\n",
    "$$\n",
    "\n",
    "This can also be used to find most similar words, given a word $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond_prob(word_idx1, word_idx2):\n",
    "    p = 0\n",
    "    for topic_idx in range(n_topics):\n",
    "        p1 = beta[str(topic_idx)][word_idx1] \n",
    "        p2 = beta[str(topic_idx)][word_idx2] \n",
    "        p += p2 * p1\n",
    "    return p\n",
    "\n",
    "def find_most_similar_words(word1, topN):\n",
    "    word_idx1 = token2int[word1]\n",
    "    cond_prob = []\n",
    "    for word2 in final_tokens:\n",
    "        word_idx2 = token2int[word2]\n",
    "        prob = get_cond_prob(word_idx1, word_idx2)\n",
    "        cond_prob.append(prob)\n",
    "    cond_prob = np.array(cond_prob) / sum(cond_prob)\n",
    "    most_similar_word_idxs = np.argsort(cond_prob)[::-1][:topN]\n",
    "    return list(map(lambda x: int2token[x], most_similar_word_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to \"mountains\":\n",
      "['notice', 'feeble', 'testimony', 'labor', 'overdriving', 'africa', 'universally', 'judge', 'unblinkingly', 'imbruing', 'stunning', 'explode', 'fellowship', 'calamity', 'fibers']\n"
     ]
    }
   ],
   "source": [
    "topN = 15\n",
    "\n",
    "word = np.random.choice(final_tokens, 1)[0]    \n",
    "most_similar_words = find_most_similar_words(word, topN)\n",
    "print(f'Most similar words to \"{word}\":')\n",
    "print(most_similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "951daa5e1959839fcb325fff331f52e72634f7a1be998f6081ed7f433b63f1b3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
